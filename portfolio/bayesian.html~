<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
<head> 
<title>Bayesian Classifier</title>
<meta name="keywords" content="" />
<meta name="description" content="" />
<meta name="author" content="" />
<meta http-equiv="content-type" content="text/html;charset=utf-8" />
<meta http-equiv="Content-Style-Type" content="text/css" />
<link rel="stylesheet" href="css/blueprint/screen.css" type="text/css" media="screen, projection" />
<link rel="stylesheet" href="css/blueprint/print.css" type="text/css" media="print" />
<link rel="stylesheet" href="css/main.css" type="text/css" media="screen" /> 
<!--[if IE]>
  <link rel="stylesheet" href="css/blueprint/ie.css" type="text/css" media="screen, projection">
<![endif]-->
</head>
<body>
<div class="container">
  <h1>Bayesian Classifier</h1>
  <h2>What are Bayesian Classifiers</h2><br>
Naive Bayesian Classifiers, often referred to just as a Bayesian Classifier, are the most nieve version of classifiers. It uses the Bayes' Theorem on conditional probability in order to classify. It essentially looks at the unclassified data and using probability of previously classified data, predicts how the unclassified data should be classified.
<br>
Bayes' Theorem is stated in an image taken from <a href="http://en.wikipedia.org/wiki/Bayes'_theorem">Wikipedia</a> where P(X) is the probability of X occuring, and P(A | B) is the probability of A occuring given B. <br>
<img src="bayes/bayes1.png">
<br>  

<br><br>  <h2>About the implementation</h2>
The following code is an example from the book Collective Intelligence. It is an implementation of a naive Bayesian classifier that is used to classify documents into different categories depending on their words. 
<br>  	<pre class="code">
# This will allow you to use the naive bayesian classified
class naivebayes(classifier):
  # Initialize the object
  def __init__(self,getfeatures):
    classifier.__init__(self,getfeatures)
    self.thresholds={}
  
  # calculate the probability that the whole item will be in category
  # (cat)
  def docprob(self,item,cat):
    features=self.getfeatures(item)   

    # Multiply the probabilities of all the features together
    p=1
    for f in features: p*=self.weightedprob(f,cat,self.fprob)
    return p

  # Calcultate the overall probability of finding a category (cat) in the
  # item 
  def prob(self,item,cat):
    catprob=self.catcount(cat)/self.totalcount()
    docprob=self.docprob(item,cat)
    return docprob*catprob
  
  # Will set the threshold
  def setthreshold(self,cat,t):
    self.thresholds[cat]=t
    
  # Will get the threshold
  def getthreshold(self,cat):
    if cat not in self.thresholds: return 1.0
    return self.thresholds[cat]
  
  # This will attempt to classify item. If it can't be classified within
  # the threshold it will return default
  def classify(self,item,default=None):
    probs={}
    # Find the category with the highest probability
    max=0.0
    for cat in self.categories():
      probs[cat]=self.prob(item,cat)
      if probs[cat]>max: 
        max=probs[cat]
        best=cat

    # Make sure the probability exceeds threshold*next best
    for cat in probs:
      if cat==best: continue
      if probs[cat]*self.getthreshold(best)>probs[best]: return default
    return best

	</pre> 

</div>
</body>
</html>

